
# Generated by Qodo Gen
from scrape_pydantic import scrape_sitemap_async
from scrape_pydantic import ScrapedPageData
import logging


# Dependencies:
# pip install pytest-mock
import pytest

class TestScrapeSitemapAsync:

    # Successfully scrapes a sitemap and returns a list of ScrapedPageData objects
    @pytest.mark.asyncio
    async def test_successful_sitemap_scrape(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"
    
        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance
    
        # Mock the result from crawler.arun()
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "https://example.com/page1", "content": "Page 1 content"},
            {"url": "https://example.com/page2", "content": "Page 2 content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result
    
        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)
    
        # Act
        result = await scrape_sitemap_async(sitemap_url)
    
        # Assert
        assert len(result) == 2
        assert isinstance(result[0], ScrapedPageData)
        assert str(result[0].url) == "https://example.com/page1"
        assert result[0].content == "Page 1 content"
        assert str(result[1].url) == "https://example.com/page2"
        assert result[1].content == "Page 2 content"
    
        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Handles empty result object from crawler.arun()
    @pytest.mark.asyncio
    async def test_handles_empty_result(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"
    
        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance
    
        # Mock an empty result from crawler.arun()
        mock_crawler_instance.arun.return_value = None
    
        # Mock logging to verify warning message
        mock_logging = mocker.patch("scrape_pydantic.logging")
    
        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)
    
        # Act
        result = await scrape_sitemap_async(sitemap_url)
    
        # Assert
        assert isinstance(result, list)
        assert len(result) == 0
    
        # Verify warning was logged
        mock_logging.warning.assert_any_call("Scraping completed but the result object was empty or None.")
    
        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Correctly processes result.results when it contains valid URL and content data
    @pytest.mark.asyncio
    async def test_processes_valid_url_and_content_data(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun()
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "https://example.com/page1", "content": "Page 1 content"},
            {"url": "https://example.com/page2", "content": "Page 2 content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert len(result) == 2
        assert isinstance(result[0], ScrapedPageData)
        assert str(result[0].url) == "https://example.com/page1"
        assert result[0].content == "Page 1 content"
        assert str(result[1].url) == "https://example.com/page2"
        assert result[1].content == "Page 2 content"

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Properly instantiates AsyncWebCrawler with verbose=True
    @pytest.mark.asyncio
    async def test_async_web_crawler_instantiation_with_verbose(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        await scrape_sitemap_async(sitemap_url)

        # Assert
        mock_crawler.assert_called_once_with(verbose=True)

    # Logs informative messages at each step of the scraping process
    @pytest.mark.asyncio
    async def test_logging_during_sitemap_scrape(self, mocker, caplog):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun()
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "https://example.com/page1", "content": "Page 1 content"},
            {"url": "https://example.com/page2", "content": "Page 2 content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        with caplog.at_level(logging.INFO):
            await scrape_sitemap_async(sitemap_url)

        # Assert
        assert "Instantiating AsyncWebCrawler..." in caplog.text
        assert f"Starting sitemap scrape for {sitemap_url} using crawl4ai library..." in caplog.text
        assert "Scraping returned 2 items. Processing..." in caplog.text
        assert "Successfully processed 2 pages." in caplog.text

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Creates valid ScrapedPageData objects with proper URL and content values
    @pytest.mark.asyncio
    async def test_creates_valid_scraped_page_data_objects(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun()
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "https://example.com/page1", "content": "Page 1 content"},
            {"url": "https://example.com/page2", "content": "Page 2 content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert len(result) == 2
        assert isinstance(result[0], ScrapedPageData)
        assert str(result[0].url) == "https://example.com/page1"
        assert result[0].content == "Page 1 content"
        assert str(result[1].url) == "https://example.com/page2"
        assert result[1].content == "Page 2 content"

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Processes result where result.results is not a list
    @pytest.mark.asyncio
    async def test_result_results_not_a_list(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun() with results not being a list
        mock_result = mocker.MagicMock()
        mock_result.results = "not_a_list"
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert len(result) == 0  # Expecting no valid ScrapedPageData objects

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Manages case when result has no 'results' attribute
    @pytest.mark.asyncio
    async def test_no_results_attribute(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun() without 'results' attribute
        mock_result = mocker.MagicMock()
        del mock_result.results  # Ensure 'results' attribute is not present
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert result == []  # Expecting an empty list since no 'results' attribute

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Manages Pydantic validation errors when creating ScrapedPageData objects
    @pytest.mark.asyncio
    async def test_pydantic_validation_error_handling(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun() with invalid URL
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "invalid-url", "content": "Page content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert len(result) == 0  # Expecting no valid ScrapedPageData objects due to validation error

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Handles exceptions during the scraping process
    @pytest.mark.asyncio
    async def test_exception_handling_during_scraping(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun() to raise an exception
        mock_crawler_instance.arun.side_effect = Exception("Test exception during scraping")

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert result == []  # Expecting an empty list due to exception

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Deals with malformed URLs that fail HttpUrl validation
    @pytest.mark.asyncio
    async def test_malformed_url_handling(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun() with a malformed URL
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "htp://malformed-url", "content": "Malformed URL content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert len(result) == 0  # Expecting no valid ScrapedPageData due to malformed URL

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Handles items with missing URL or content fields
    @pytest.mark.asyncio
    async def test_handles_missing_url_or_content(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun()
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "https://example.com/page1", "content": "Page 1 content"},
            {"url": None, "content": "Missing URL"},
            {"url": "https://example.com/page3", "content": None},
            {"url": "https://example.com/page4", "content": "Page 4 content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert len(result) == 2
        assert isinstance(result[0], ScrapedPageData)
        assert str(result[0].url) == "https://example.com/page1"
        assert result[0].content == "Page 1 content"
        assert str(result[1].url) == "https://example.com/page4"
        assert result[1].content == "Page 4 content"

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Provides detailed logging for debugging and monitoring
    @pytest.mark.asyncio
    async def test_logging_during_sitemap_scrape(self, mocker, caplog):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun()
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "https://example.com/page1", "content": "Page 1 content"},
            {"url": "https://example.com/page2", "content": "Page 2 content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        with caplog.at_level(logging.INFO):
            await scrape_sitemap_async(sitemap_url)

        # Assert
        assert "Instantiating AsyncWebCrawler..." in caplog.text
        assert f"Starting sitemap scrape for {sitemap_url} using crawl4ai library..." in caplog.text
        assert "Scraping returned 2 items. Processing..." in caplog.text
        assert "Successfully processed 2 pages." in caplog.text

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Uses fallback to 'markdown' field if 'content' is missing
    @pytest.mark.asyncio
    async def test_fallback_to_markdown_when_content_missing(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun()
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "https://example.com/page1", "markdown": "Page 1 markdown"},
            {"url": "https://example.com/page2", "content": "Page 2 content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert len(result) == 2
        assert isinstance(result[0], ScrapedPageData)
        assert str(result[0].url) == "https://example.com/page1"
        assert result[0].content == "Page 1 markdown"
        assert str(result[1].url) == "https://example.com/page2"
        assert result[1].content == "Page 2 content"

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Safely accesses dictionary fields using .get() with defaults
    @pytest.mark.asyncio
    async def test_safely_accesses_dictionary_fields_with_defaults(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun()
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "https://example.com/page1"},  # Missing 'content', should default to ""
            {"url": "https://example.com/page2", "markdown": "Page 2 markdown"}  # Missing 'content', should use 'markdown'
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert len(result) == 2
        assert isinstance(result[0], ScrapedPageData)
        assert str(result[0].url) == "https://example.com/page1"
        assert result[0].content == ""  # Default to empty string
        assert str(result[1].url) == "https://example.com/page2"
        assert result[1].content == "Page 2 markdown"  # Use 'markdown' as fallback

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Gracefully handles and logs different types of exceptions
    @pytest.mark.asyncio
    async def test_exception_handling_during_scraping(self, mocker, caplog):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun() to raise an exception
        mock_crawler_instance.arun.side_effect = Exception("Test exception during arun")

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        with caplog.at_level(logging.ERROR):
            result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert result == []
        assert "Error during scraping or processing: Test exception during arun" in caplog.text

        # Verify the crawler was called correctly
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)

    # Properly uses async context manager for resource cleanup
    @pytest.mark.asyncio
    async def test_async_context_manager_usage(self, mocker):
        # Arrange
        sitemap_url = "https://example.com/sitemap.xml"

        # Mock AsyncWebCrawler and its context manager
        mock_crawler = mocker.AsyncMock()
        mock_crawler_instance = mocker.AsyncMock()
        mock_crawler.return_value.__aenter__.return_value = mock_crawler_instance

        # Mock the result from crawler.arun()
        mock_result = mocker.MagicMock()
        mock_result.results = [
            {"url": "https://example.com/page1", "content": "Page 1 content"}
        ]
        mock_crawler_instance.arun.return_value = mock_result

        mocker.patch("scrape_pydantic.AsyncWebCrawler", mock_crawler)

        # Act
        result = await scrape_sitemap_async(sitemap_url)

        # Assert
        assert len(result) == 1
        assert isinstance(result[0], ScrapedPageData)
        assert str(result[0].url) == "https://example.com/page1"
        assert result[0].content == "Page 1 content"

        # Verify the crawler was called correctly and context manager was used
        mock_crawler_instance.arun.assert_called_once_with(url=sitemap_url)
        mock_crawler.return_value.__aenter__.assert_called_once()
        mock_crawler.return_value.__aexit__.assert_called_once()